{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Resampling Methods\n",
    "Covers resampling data through bootstraping and cross validation. Cross validation gets us an error estimate for our test data and boostraping provides estimates for parameter accuracy.\n",
    "\n",
    "### Cross Validation\n",
    "Usually a test set is not available so a simple strategy to create one is to split the available data into training and testing (validation set). For quantitative responses usually use MSE, for categorical can use error rate, area under the curve, F1 score, weighting of confusion matrix, etc...\n",
    "\n",
    "### Leave One Out Cross Validation\n",
    "LOOCV has only one observation in the test set and uses all other n-1 observations to build a model. n different models are built leaving out each observation once and error is averaged over these n trials.  LOOCV is better than simple method above. Model is built on nearly all the data and there is no randomness in the splits since each observation will be left out once. It is computationally expensive especially with large n and a complex model.\n",
    "\n",
    "### k-fold cross validation\n",
    "Similar to LOOCV but this time you leave some number greater than 1 out. Here, k is the number of partitions of your sample, so if you have 1000 observations and k = 10, the each fold will be 100. These 100 observations would act as your test set. Get an MSE for each fold of these 100 observations and take the average. LOOCV is a special case of k-fold CV whenever k equals the number of observations.\n",
    "\n",
    "### bias-variance tradeoff between LOOCV and k-folds\n",
    "Since LOOCV trains on nearly all the data, the test error rate will generally be lower than k-fold and there for less biased. LOOCV will have higher variance since all n models will be very highly correlated to one another. Since the models won't differ much, the test error rate (which what CV is measuring) will vary more than k-fold which has fewer models that are less correlated with one another. A value of k between 5 and 10 is a good rule of thumb that balances the trade-off between bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "Minimize $$Var(\\alpha X + (1 - \\alpha)Y)$$\n",
    "\n",
    "Properties of variance and covariance\n",
    "$$=Var(\\alpha X) + Var((1 - \\alpha)Y) + 2Cov(\\alpha X, (1 - \\alpha)Y)$$\n",
    "$$=\\alpha^2Var(X) + (1 - \\alpha)^2Var(Y) + 2(\\alpha)(1 - \\alpha)(Cov(X, Y)$$\n",
    "\n",
    "Take derivative and set to 0\n",
    "$$2\\alpha Var(X) - 2(1 - \\alpha)Var(Y) + (2 - 4\\alpha)Cov(X, Y) = 0$$\n",
    "Collect terms\n",
    "$$2\\alpha Var(X) + 2 \\alpha Var(Y) - 4\\alpha Cov(X, Y) = 2Var(Y) - 2Cov(X, Y)$$\n",
    "Solve for $\\alpha$\n",
    "$$\\alpha = \\frac{Var(Y) - Cov(X, Y)}{Var(X) + Var(Y) - 2Cov(X, Y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "a) $\\frac{n-1}{n}$  \n",
    "b) $\\frac{n-1}{n}$  \n",
    "c) Since bootstrapping is sampling with replace, the probability of being any jth obsevation is $\\frac{1}{n}$. The probability of not being the jth observation is $1 - \\frac{1}{n}$. Since each draw is independent we can just multiply the probabilities together to get the probability that the jth observation is not in the sample at all\n",
    "\n",
    "d)-f) Porbability that jth observation is in the bootstrap sample is $$Pr(in) = 1 - Pr(out)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=5, probability that jth observation is in the bootstrap sample =67.23%\n",
      "n=100, probability that jth observation is in the bootstrap sample =63.40%\n",
      "n=10000, probability that jth observation is in the bootstrap sample =63.21%\n",
      "n=100000, probability that jth observation is in the bootstrap sample =63.21%\n"
     ]
    }
   ],
   "source": [
    "#2 c-f\n",
    "list_n = [5, 100, 10000, 100000]\n",
    "for n in list_n:\n",
    "    print(\"n={}, probability that jth observation is in the bootstrap sample ={:.2%}\".format(n, (1-(1-1/n)**n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6321223982317534\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2g\n",
    "x = np.arange(1, 100001)\n",
    "y = 1-(1 - 1/x) ** x\n",
    "# plt.plot(x[:10],y[:10])\n",
    "plt.plot(x,y)\n",
    "print(min(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the first point has probability equal to one, which makes sense because if we only have on observation it must be in the bootstrap sample. After, the probability starts decreasing until it reaches a plateau close to the value 0.63.\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\left( 1- \\left(1-\\frac{1}{n}\\right)^n \\right) = 1- e^{-1} \\approx 0.63212$$\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6301\n"
     ]
    }
   ],
   "source": [
    "# 2h\n",
    "import random\n",
    "n = 100\n",
    "N = 10000\n",
    "count = 0\n",
    "\n",
    "for _ in range(N):\n",
    "    # True equals 1 in Python\n",
    "    # choices(l, k=k) takes k samples from the list l with replacement\n",
    "    count += 4 in random.choices(range(1,n+1), k=n) \n",
    "\n",
    "print(count/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.randint(1, 101, (100, 10000))\n",
    "np.any(data == 4, axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 k-folder\n",
    "a) K-fold CV works by taking the dataset given and randomly splitting it into k non-overlapping datasets. You can shuffle the data first and then just split at regular intervals. Train K models. For each model, use the kth region as the validation set and build on the other k-1 sets. Take the mean of the k errors found to estimate the true test error.  \n",
    "\n",
    "b i) Advantage to validation set is that there are more test sets to validate on which should **reduce the bias** of what the overall error actually is. Variance should also decrease as the validation set approach is just one split of the data and that split could not represent the test data well. Disadvantage is training more models.  \n",
    "\n",
    "b ii) Advantage to LOOCV is a **decrease in variance** as the k models are not as highly correlated as the each LOOCV model is. Also, K-folds is computationally less expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Default dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "      <th>student_yes</th>\n",
       "      <th>default_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance        income  student_yes  default_yes\n",
       "1      No      No   729.526495  44361.625074            0            0\n",
       "2      No     Yes   817.180407  12106.134700            1            0\n",
       "3      No      No  1073.549164  31767.138947            0            0\n",
       "4      No      No   529.250605  35704.493935            0            0\n",
       "5      No      No   785.655883  38463.495879            0            0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default = pd.read_excel('Data/Default.xlsx')\n",
    "default['student_yes'] = (default['student'] == 'Yes').astype('int')\n",
    "default['default_yes'] = (default['default'] == 'Yes').astype('int')\n",
    "default.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for LogisticRegression without regularization. In sklearn this is not implemented, but we can use l2 regularization and set C, the inverese strenght, to a very high number, effectively removing the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:  [-11.5273343]\n",
      "Coefficients:  [[2.07267113e-05 5.64079143e-03]]\n"
     ]
    }
   ],
   "source": [
    "X = default[['income', 'balance']]\n",
    "y = default['default_yes']\n",
    "\n",
    "model = LogisticRegression(C=10**6, tol=1e-6)\n",
    "model.fit(X,y)\n",
    "print('Intercept: ', model.intercept_)\n",
    "print('Coefficients: ', model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>default_yes</td>   <th>  No. Observations:  </th>   <td> 10000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  9997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Mon, 07 Jan 2019</td> <th>  Pseudo R-squ.:     </th>   <td>0.4594</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>23:55:03</td>     <th>  Log-Likelihood:    </th>  <td> -789.48</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -1460.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>4.541e-292</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>  -11.5405</td> <td>    0.435</td> <td>  -26.544</td> <td> 0.000</td> <td>  -12.393</td> <td>  -10.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>    <td> 2.081e-05</td> <td> 4.99e-06</td> <td>    4.174</td> <td> 0.000</td> <td>  1.1e-05</td> <td> 3.06e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th>   <td>    0.0056</td> <td>    0.000</td> <td>   24.835</td> <td> 0.000</td> <td>    0.005</td> <td>    0.006</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.14 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:            default_yes   No. Observations:                10000\n",
       "Model:                          Logit   Df Residuals:                     9997\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Mon, 07 Jan 2019   Pseudo R-squ.:                  0.4594\n",
       "Time:                        23:55:03   Log-Likelihood:                -789.48\n",
       "converged:                       True   LL-Null:                       -1460.3\n",
       "                                        LLR p-value:                4.541e-292\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
       "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
       "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using statsmodel\n",
    "f = 'default_yes ~ income+ balance'\n",
    "res = smf.logit(formula = f, data= default).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032200000000000006\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=7)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "test_error = 1-(y_test==y_pred).mean()\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0252\n",
      "0.026800000000000046\n",
      "0.02200000000000002\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=100000, tol=.0000001)\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_error = 1-(y_test==y_pred).mean()\n",
    "    print(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test error changes when different train test split have been made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028000000000000025\n",
      "0.029200000000000004\n",
      "0.024800000000000044\n"
     ]
    }
   ],
   "source": [
    "X = default[['income', 'balance','student_yes']]\n",
    "y = default['default_yes']\n",
    "model = LogisticRegression(C=10**6, tol=1e-6)\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_error = 1-(y_test==y_pred).mean()\n",
    "    print(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So compared to the values above without the student dummy variable, it seems that adding the student variable does not help in any of the metrics since they are worse or very similar (although we should consider the variance, in a more careful analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Generalized Linear Model Regression Results                        \n",
      "===========================================================================================\n",
      "Dep. Variable:     ['default[No]', 'default[Yes]']   No. Observations:                10000\n",
      "Model:                                         GLM   Df Residuals:                     9997\n",
      "Model Family:                             Binomial   Df Model:                            2\n",
      "Link Function:                               logit   Scale:                          1.0000\n",
      "Method:                                       IRLS   Log-Likelihood:                -789.48\n",
      "Date:                             Mon, 07 Jan 2019   Deviance:                       1579.0\n",
      "Time:                                     23:55:03   Pearson chi2:                 6.95e+03\n",
      "No. Iterations:                                  9   Covariance Type:             nonrobust\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     11.5405      0.435     26.544      0.000      10.688      12.393\n",
      "income     -2.081e-05   4.99e-06     -4.174      0.000   -3.06e-05    -1.1e-05\n",
      "balance       -0.0056      0.000    -24.835      0.000      -0.006      -0.005\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "#using generalized linear models with statsmodel\n",
    "#see the wikipedia reference to understand why family is binomial\n",
    "mod1 = smf.glm(formula='default ~ income + balance', data=default, family=sm.families.Binomial()).fit() #create & fit model\n",
    "print(mod1.summary()) #show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_fn(default):\n",
    "    mod = smf.glm(formula='default_yes ~ income + balance', data=default, family=sm.families.Binomial()).fit() \n",
    "    coef_income = mod1.params[1]\n",
    "    coef_balance = mod1.params[2]\n",
    "    return [coef_income, coef_balance]\n",
    "\n",
    "def boot(X, bootSample_size=None):\n",
    "\n",
    "    #assign default size if non-specified\n",
    "    if bootSample_size == None:\n",
    "        bootSample_size = len(X)\n",
    "\n",
    "    #create random integers to use as indices for bootstrap sample based on original data\n",
    "    bootSample_i = (np.random.rand(bootSample_size)*len(X)).astype(int)\n",
    "    bootSample_i = np.array(bootSample_i)\n",
    "    bootSample_X = X.iloc[bootSample_i]\n",
    "    return bootSample_X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.000021\n",
      "1   -0.005647\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#running model for bootstrapped samples\n",
    "coefficients = [] #variable initialization\n",
    "n = 100 #number of bootstrapped samples\n",
    "\n",
    "for i in range(n):\n",
    "    coef_i = boot_fn(boot(default)) #determining coefficients for specific bootstrapped sample\n",
    "    coefficients.append(coef_i) #saving coefficients value\n",
    "\n",
    "print(pd.DataFrame(coefficients).mean()) #print average of coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Leave one out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Direction_Up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-3.484</td>\n",
       "      <td>0.154976</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>Down</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.148574</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>Down</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>0.159837</td>\n",
       "      <td>3.514</td>\n",
       "      <td>Up</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>0.161630</td>\n",
       "      <td>0.712</td>\n",
       "      <td>Up</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990</td>\n",
       "      <td>0.712</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.153728</td>\n",
       "      <td>1.178</td>\n",
       "      <td>Up</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5    Volume  Today Direction  \\\n",
       "0  1990  0.816  1.572 -3.936 -0.229 -3.484  0.154976 -0.270      Down   \n",
       "1  1990 -0.270  0.816  1.572 -3.936 -0.229  0.148574 -2.576      Down   \n",
       "2  1990 -2.576 -0.270  0.816  1.572 -3.936  0.159837  3.514        Up   \n",
       "3  1990  3.514 -2.576 -0.270  0.816  1.572  0.161630  0.712        Up   \n",
       "4  1990  0.712  3.514 -2.576 -0.270  0.816  0.153728  1.178        Up   \n",
       "\n",
       "   Direction_Up  \n",
       "0             0  \n",
       "1             0  \n",
       "2             1  \n",
       "3             1  \n",
       "4             1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly = pd.read_csv('Data/weekly.csv')\n",
    "weekly['Direction_Up'] = (weekly['Direction']=='Up').astype(int)\n",
    "weekly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22122405] [[-0.03872222  0.0602483 ]] 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "# a) Fit a logistic regression model that predicts Direction using Lag1 and Lag2.\n",
    "X = weekly[['Lag1', 'Lag2']]\n",
    "y = weekly.Direction_Up\n",
    "mod = LogisticRegression(C=10**6, tol=10**-7)\n",
    "mod.fit(X, y)\n",
    "print(mod.intercept_, mod.coef_, (mod.predict(X) == y).mean())  # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22324305] [[-0.03843317  0.06084763]] 0.5564738292011019\n"
     ]
    }
   ],
   "source": [
    "# b) Fit a logistic regression model that predicts Direction using Lag1 and \n",
    "# Lag2 using all but the first observation.\n",
    "mod = LogisticRegression(C=10**6, tol=10**-7)\n",
    "mod.fit(X.iloc[1:], y.iloc[1:])\n",
    "print(mod.intercept_, mod.coef_, (mod.predict(X) == y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction = [1], ground truth=0\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction = {}, ground truth={}\".format(mod.predict(X.iloc[[0]]),y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is misclasiffied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44995408631772266\n"
     ]
    }
   ],
   "source": [
    "n = len(X)\n",
    "errors = np.zeros(n)\n",
    "for i in range(n):\n",
    "    one_out = ~X.index.isin([i])\n",
    "    mod.fit(X[one_out], y[one_out])\n",
    "    if mod.predict(X.iloc[[i]])!=y[i]:\n",
    "        errors[i]=1\n",
    "print(errors.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "y = np.random.normal(size = 100)\n",
    "x = np.random.normal(size = 100)\n",
    "epsilon = np.random.normal(size=100)\n",
    "y= x-2*x**2 + epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, n=100 and p = number of predictors = 2 ( x and $x^2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1c1d6082b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quadratic plot\n",
    "- Convex function with negative concavity\n",
    "- X from about -2 to 2\n",
    "- Y from about -10 to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import Pipeline: \n",
    "Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (MSE): 8.292212\n",
      "Model 2 (MSE): 1.017096\n",
      "Model 3 (MSE): 1.046553\n",
      "Model 4 (MSE): 1.057493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "# Organize data in dataframe\n",
    "df = pd.DataFrame({'x':x, 'y':y})\n",
    "min_deg = 1\n",
    "max_deg = 4+1\n",
    "scores = []\n",
    "for i in range(min_deg,max_deg):\n",
    "    for train_idx, test_idx in loo.split(df):\n",
    "        X_train, X_test = df['x'][train_idx], df['x'][test_idx]\n",
    "        y_train, y_test = df['y'][train_idx], df['y'][test_idx]\n",
    "        model = Pipeline([('poly', PolynomialFeatures(degree = i)),\n",
    "                      ('linear', LinearRegression())])\n",
    "        model.fit(X_train[:,np.newaxis], y_train)\n",
    "\n",
    "        # MSE\n",
    "        score = mean_squared_error(y_test, model.predict(X_test[:,np.newaxis]))\n",
    "        scores.append(score)\n",
    "    print('Model %i (MSE): %f' % (i,np.mean(scores)))\n",
    "    scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.088\n",
      "Model:                            OLS   Adj. R-squared:                  0.079\n",
      "Method:                 Least Squares   F-statistic:                     9.460\n",
      "Date:                Mon, 07 Jan 2019   Prob (F-statistic):            0.00272\n",
      "Time:                        23:55:10   Log-Likelihood:                -242.69\n",
      "No. Observations:                 100   AIC:                             489.4\n",
      "Df Residuals:                      98   BIC:                             494.6\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -1.7609      0.280     -6.278      0.000      -2.317      -1.204\n",
      "x1             0.9134      0.297      3.076      0.003       0.324       1.503\n",
      "==============================================================================\n",
      "Omnibus:                       40.887   Durbin-Watson:                   1.957\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               83.786\n",
      "Skew:                          -1.645   Prob(JB):                     6.40e-19\n",
      "Kurtosis:                       6.048   Cond. No.                         1.19\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.882\n",
      "Model:                            OLS   Adj. R-squared:                  0.880\n",
      "Method:                 Least Squares   F-statistic:                     362.9\n",
      "Date:                Mon, 07 Jan 2019   Prob (F-statistic):           9.26e-46\n",
      "Time:                        23:55:10   Log-Likelihood:                -140.40\n",
      "No. Observations:                 100   AIC:                             286.8\n",
      "Df Residuals:                      97   BIC:                             294.6\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0216      0.122     -0.177      0.860      -0.264       0.221\n",
      "x1             1.2132      0.108     11.238      0.000       0.999       1.428\n",
      "x2            -2.0014      0.078    -25.561      0.000      -2.157      -1.846\n",
      "==============================================================================\n",
      "Omnibus:                        0.094   Durbin-Watson:                   2.221\n",
      "Prob(Omnibus):                  0.954   Jarque-Bera (JB):                0.009\n",
      "Skew:                          -0.022   Prob(JB):                        0.995\n",
      "Kurtosis:                       2.987   Cond. No.                         2.26\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.883\n",
      "Model:                            OLS   Adj. R-squared:                  0.880\n",
      "Method:                 Least Squares   F-statistic:                     242.1\n",
      "Date:                Mon, 07 Jan 2019   Prob (F-statistic):           1.26e-44\n",
      "Time:                        23:55:10   Log-Likelihood:                -139.91\n",
      "No. Observations:                 100   AIC:                             287.8\n",
      "Df Residuals:                      96   BIC:                             298.2\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0046      0.125      0.037      0.971      -0.244       0.253\n",
      "x1             1.0639      0.189      5.636      0.000       0.689       1.439\n",
      "x2            -2.0215      0.081    -24.938      0.000      -2.182      -1.861\n",
      "x3             0.0550      0.057      0.965      0.337      -0.058       0.168\n",
      "==============================================================================\n",
      "Omnibus:                        0.034   Durbin-Watson:                   2.253\n",
      "Prob(Omnibus):                  0.983   Jarque-Bera (JB):                0.050\n",
      "Skew:                           0.032   Prob(JB):                        0.975\n",
      "Kurtosis:                       2.911   Cond. No.                         6.55\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.885\n",
      "Model:                            OLS   Adj. R-squared:                  0.880\n",
      "Method:                 Least Squares   F-statistic:                     182.4\n",
      "Date:                Mon, 07 Jan 2019   Prob (F-statistic):           1.13e-43\n",
      "Time:                        23:55:10   Log-Likelihood:                -139.24\n",
      "No. Observations:                 100   AIC:                             288.5\n",
      "Df Residuals:                      95   BIC:                             301.5\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0866      0.144      0.600      0.550      -0.200       0.373\n",
      "x1             1.0834      0.189      5.724      0.000       0.708       1.459\n",
      "x2            -2.2455      0.214    -10.505      0.000      -2.670      -1.821\n",
      "x3             0.0436      0.058      0.755      0.452      -0.071       0.158\n",
      "x4             0.0482      0.043      1.132      0.260      -0.036       0.133\n",
      "==============================================================================\n",
      "Omnibus:                        0.102   Durbin-Watson:                   2.214\n",
      "Prob(Omnibus):                  0.950   Jarque-Bera (JB):                0.117\n",
      "Skew:                           0.069   Prob(JB):                        0.943\n",
      "Kurtosis:                       2.906   Cond. No.                         17.5\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "min_deg = 1  \n",
    "max_deg = 4+1 \n",
    "\n",
    "for i in range(min_deg, max_deg):\n",
    "    pol = PolynomialFeatures(degree = i)\n",
    "    X_pol = pol.fit_transform(df['x'][:,np.newaxis])\n",
    "    y = df['y']\n",
    "\n",
    "    model = sm.OLS(y, X_pol)\n",
    "    results = model.fit()\n",
    "\n",
    "    print(results.summary())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "    black  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = pd.read_csv('Data/boston.csv')\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.532806324110698\n"
     ]
    }
   ],
   "source": [
    "medv = boston.medv\n",
    "mu = medv.mean()\n",
    "print(medv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard eror =  0.4088611474975351\n"
     ]
    }
   ],
   "source": [
    "# Standard error = standard deviation / sqrt(number of observation)\n",
    "SE = medv.std() / np.sqrt(len(medv))\n",
    "print(\"Standard eror = \", SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4190510584687809"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate the SE of mu using bootstrap\n",
    "means = [medv.sample(n = len(medv), replace=True).mean() for _ in range(1000)]\n",
    "np.std(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.694704207173135 23.37090844104826\n"
     ]
    }
   ],
   "source": [
    "# 95% CI\n",
    "SE = np.std(means)\n",
    "print(mu - 2*SE, mu + 2*SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# median\n",
    "medv.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39181342498694416"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SE of median\n",
    "medians = [medv.sample(n = len(medv), replace=True).median() for _ in range(1000)]\n",
    "np.std(medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.75"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantile\n",
    "medv.quantile(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48166647952706865"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SE of mu_0.1\n",
    "quantiles = [medv.sample(n = len(medv), replace=True).quantile(0.1) for _ in range(1000)]\n",
    "np.std(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
